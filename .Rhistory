activityData$date<-as.Date(activityData$date)
##provide a summary of the activityData table
summary(activityData)
## variables for documenting the observations and variables for the activity dataset
obs<-nrow(activityData)
vars<-ncol(activityData)
activityDataComplete <- na.omit(activityData)
activityDataComplete$date <- as.factor(activityDataComplete$date)
##provide a summary of the activityDataComplete table
summary(activityDataComplete)
## variables for documenting the observations and variables for the activity dataset
obs2<-nrow(activityDataComplete)
vars2<-ncol(activityDataComplete)
calcActivity <- activityDataComplete %.%
group_by(date) %.%
summarise (step.sum = sum(steps), step.mean = round(mean(steps)), step.median=median(steps))
calcActivity
dMean <- round(mean(calcActivity$step.sum))
dMedian <- median(calcActivity$step.sum)
p <- ggplot(calcActivity, aes(x=step.sum)) + geom_histogram(binwidth=750, colour="black", fill="#bbaaff")
p <- p+geom_vline(aes(xintercept=dMean), color="#ff1100", size = 10, linetype = "solid", alpha=0.33)
p <- p+geom_vline(aes(xintercept=dMedian), color="#0000ff", linetype = "dashed", size=1, alpha=1)
p <-  p+ggtitle("A Histogram of the Total Step Counts per Day from October ~ November")+ theme(plot.title = element_text(lineheight=.8, face="bold"))
p<-p+xlab("Step Count Daily Totals") + ylab("Frequency of Step Count Totals")
p <- p+geom_text(aes(dMean,0,label = "mean =", hjust=-1, vjust = -29))
p <- p+geom_text(aes(dMean,0,label = dMean, hjust=-2.5, vjust = -29))
p <- p+geom_text(aes(dMedian,0,label = dMedian, hjust=-2.5, vjust = -27))
p <- p+geom_text(aes(dMedian,0,label = "median = ", hjust=-0.6, vjust = -27))
p
##make a new complete and clean data set
activityDataComplete2 <- na.omit(activityData)
## convert the interval col to a factor
activityDataComplete2$interval <- as.factor(activityDataComplete2$interval)
##group by interval factor and average
calcActivity2 <- activityDataComplete2 %.%
group_by(interval) %.%
summarise (step.mean = mean(steps))
calcActivity2
l <- ggplot(data=calcActivity2, aes(x=as.numeric(levels(interval))[interval]/100, y=step.mean, group=1)) + geom_line(colour="#000099", linetype="solid", size=1.4) + geom_point(colour="#bbaaff", size=1.1, shape=21, fill="#EB92F7")
l <- l + scale_x_continuous(breaks=c(0, 4, 8, 12, 16, 20, 24))
l <- l + ggtitle("Average Number of Steps per Interval from October ~ November")+ theme(plot.title = element_text(lineheight=.8, face="bold"))
l <- l + xlab("Step Count Recording Interval (hour)") + ylab("Average Step Count")
l
calcActivity2[which.max(calcActivity2$step.mean),]
missingValues<-activityData[which(is.na(activityData)),]
nrow(missingValues)
ad<- activityData
for(i in 1:nrow(ad)){
if(is.na(ad[i,]$steps)){
tmp<-ad[i,]$interval
ad[i,]$steps <- calcActivity2[which(calcActivity2$interval==tmp),]$step.mean
}
}
summary(ad)
calcActivity3 <- ad %.%
group_by(date) %.%
summarise (step.sum = sum(steps), step.mean = round(mean(steps)), step.median=median(steps))
calcActivity3
dMean3 <- round(mean(calcActivity3$step.sum))
dMedian3 <- round(median(calcActivity3$step.sum))
p <- ggplot(calcActivity3, aes(x=step.sum)) + geom_histogram(binwidth=750, colour="black", fill="#bbaaff")
p <- p+geom_vline(aes(xintercept=dMean), color="#ff1100", size = 10, linetype = "solid", alpha=0.33)
p <- p+geom_vline(aes(xintercept=dMedian), color="#0000ff", linetype = "dashed", size=1, alpha=1)
p <-  p+ggtitle("A Histogram of the Total Step Counts per Day from October ~ November")+ theme(plot.title = element_text(lineheight=.8, face="bold"))
p<-p+xlab("Step Count Daily Totals") + ylab("Frequency of Step Count Totals")
p <- p+geom_text(aes(dMean,0,label = "mean =", hjust=-1, vjust = -29))
p <- p+geom_text(aes(dMean,0,label = dMean, hjust=-2.5, vjust = -29))
p <- p+geom_text(aes(dMedian,0,label = dMedian, hjust=-2.5, vjust = -27))
p <- p+geom_text(aes(dMedian,0,label = "median = ", hjust=-0.6, vjust = -27))
p
ad$date <- as.Date(ad$date)
class(ad$date)
tmp2<-ad
##mutate a new col using weekdays (POSIX weekday Sunday=0, Saturday=6), an inline ifelse and dplyr
tmp3 <- mutate(tmp2, day = ifelse(as.POSIXlt(ad$date)$wday==0 | as.POSIXlt(ad$date)$wday==6, "weekend", "weekday"))
## convert to a factor
tmp3$day<-as.factor(tmp3$day)
##provide a summary to confirm
summary(tmp3)
##group by interval factor and average
tmp3$interval <- as.factor(tmp3$interval)
calcActivity5 <- tmp3 %.%
filter(day=="weekend") %.%
group_by(interval) %.%
summarise (step.mean = mean(steps))
calcActivity5
calcActivity4 <- tmp3 %.%
filter(day=="weekday") %.%
group_by(interval) %.%
summarise (step.mean = mean(steps))
calcActivity4
l <- ggplot(data=calcActivity5, aes(x=as.numeric(levels(interval))[interval]/100, y=step.mean, group=1)) + geom_line(colour="#000099", linetype="solid", size=1.4) + geom_point(colour="#bbaaff", size=1.1, shape=21, fill="#EB92F7")
l <- l + scale_x_continuous(breaks=c(0, 4, 8, 12, 16, 20, 24))
l <- l + ggtitle("Average Number of Steps per Interval on Weekends")+ theme(plot.title = element_text(lineheight=.8, face="bold"))
l <- l + xlab("Step Count Recording Interval (hour)") + ylab("Average Step Count")
r <- ggplot(data=calcActivity4, aes(x=as.numeric(levels(interval))[interval]/100, y=step.mean, group=1)) + geom_line(colour="#000099", linetype="solid", size=1.4) + geom_point(colour="#bbaaff", size=1.1, shape=21, fill="#EB92F7")
r <- r + scale_x_continuous(breaks=c(0, 4, 8, 12, 16, 20, 24))
r <- r + ggtitle("Average Number of Steps per Interval on Weekdays")+ theme(plot.title = element_text(lineheight=.8, face="bold"))
r <- r + xlab("Step Count Recording Interval (hour)") + ylab("Average Step Count")
grid.arrange(l, r, nrow=2, main="Average Weekend vs. Weekday activity patterns")
grid.arrange(l, r, nrow=2, ncol=1, main="Average Weekend vs. Weekday activity patterns")
grid.arrange(l, r, ncol=1, main="Average Weekend vs. Weekday activity patterns")
grid.arrange(l, r, nrow=1, main="Average Weekend vs. Weekday activity patterns")
grid.arrange(l, r, ncols=1)
grid.arrange(l, r, ncol=1)
grid.arrange(l, r, nrow=1)
grid.arrange(l, r, ncol=1)
grid.arrange(l, r, ncol=1, main="Average Weekend vs. Weekday activity patterns")
grid.arrange(l, r, ncol=1)
?grid.arrange
grid.arrange(l, r, ncol=1, top="Test")
grid.arrange(l, r, ncol=1, top="Average Weekend vs. Weekday activity patterns")
calcActivity <- activityDataComplete %.%
group_by(date) %.%
summarise (step.sum = sum(steps), step.mean = round(mean(steps)), step.median=median(steps))
calcActivity
calcActivity <- activityDataComplete %>%
group_by(date) %>%
summarise (step.sum = sum(steps), step.mean = round(mean(steps)), step.median=median(steps))
calcActivity
install.packages('RWordPress', repos = 'http://www.omegahat.org/R', type = 'source')
---
title: "Regression Models Course Project - Motor Trend Data Analysis Report"
author: "Alexander Loth"
output:
pdf_document:
fig_height: 6
fig_width: 7
---
## Abstract
In this report, we will analyze the `mtcars` data set and explore the relationship between a set of variables and miles per gallon (MPG). The data was extracted from the 1974 *Motor Trend* US magazine, and comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973â€“74 models). We use regression models and exploratory data analyses to mainly explore how **automatic** (am = 0) and **manual** (am = 1) transmissions features affect the **MPG** feature. The t-test shows that the performance difference between cars with automatic and manual transmission. And it is about 7 MPG more for cars with manual transmission than those with automatic transmission. Then, we fit several linear regression models and select the one with highest Adjusted R-squared value. So, given that weight and 1/4 mile time are held constant, manual transmitted cars are 14.079 + (-4.141)*weight more MPG on average better than automatic transmitted cars. Thus, cars that are lighter in weight with a manual transmission and cars that are heavier in weight with an automatic transmission will have higher MPG values.
## Exploratory Data Analysis
First, we load the `mtcars` data set and change some variables from `numeric` class to `factor` class.
```{r}
library(ggplot2)
data(mtcars)
mtcars[1:3, ] # Sample Data
dim(mtcars)
mtcars$cyl <- as.factor(mtcars$cyl)
mtcars$vs <- as.factor(mtcars$vs)
mtcars$am <- factor(mtcars$am)
mtcars$gear <- factor(mtcars$gear)
mtcars$carb <- factor(mtcars$carb)
attach(mtcars)
```
Then, we do some basic exploratory data analyses. Please refer to the **Appendix: Figures** section for the plots. According to the box plot, we see that manual transmission yields higher values of MPG in general. And as for the pair graph, we can see some higher correlations between variables like "wt", "disp", "cyl" and "hp".
## Inference
At this step, we make the null hypothesis as the MPG of the automatic and manual transmissions are from the same population (assuming the MPG has a normal distribution). We use the two sample T-test to show it.
```{r}
result <- t.test(mpg ~ am)
result$p.value
result$estimate
```
Since the p-value is 0.00137, we reject our null hypothesis. So, the automatic and manual transmissions are from different populations. And the mean for MPG of manual transmitted cars is about 7 more than that of automatic transmitted cars.
## Regression Analysis
First, we fit the full model as the following.
```{r, results='hide'}
fullModel <- lm(mpg ~ ., data=mtcars)
summary(fullModel) # results hidden
```
This model has the Residual standard error as 2.833 on 15 degrees of freedom. And the Adjusted R-squared value is 0.779, which means that the model can explain about 78% of the variance of the MPG variable. However, none of the coefficients are significant at 0.05 significant level.
Then, we use backward selection to select some statistically significant variables.
```{r, results='hide'}
stepModel <- step(fullModel, k=log(nrow(mtcars)))
summary(stepModel) # results hidden
```
This model is "mpg ~ wt + qsec + am". It has the Residual standard error as 2.459 on 28 degrees of freedom. And the Adjusted R-squared value is 0.8336, which means that the model can explain about 83% of the variance of the MPG variable. All of the coefficients are significant at 0.05 significant level.
Please refer to the **Appendix: Figures** section for the plots again. According to the scatter plot, it indicates that there appear to be an interaction term between "wt" variable and "am" variable, since automatic cars tend to weigh heavier than manual cars. Thus, we have the following model including the interaction term:
```{r, results='hide'}
amIntWtModel<-lm(mpg ~ wt + qsec + am + wt:am, data=mtcars)
summary(amIntWtModel) # results hidden
```
This model has the Residual standard error as 2.084 on 27 degrees of freedom. And the Adjusted R-squared value is 0.8804, which means that the model can explain about 88% of the variance of the MPG variable. All of the coefficients are significant at 0.05 significant level. This is a pretty good one.
Next, we fit the simple model with MPG as the outcome variable and Transmission as the predictor variable.
```{r, results='hide'}
amModel<-lm(mpg ~ am, data=mtcars)
summary(amModel) # results hidden
```
It shows that on average, a car has 17.147 mpg with automatic transmission, and if it is manual transmission, 7.245 mpg is increased. This model has the Residual standard error as 4.902 on 30 degrees of freedom. And the Adjusted R-squared value is 0.3385, which means that the model can explain about 34% of the variance of the MPG variable. The low Adjusted R-squared value also indicates that we need to add other variables to the model.
Finally, we select the final model.
```{r, results='hide'}
anova(amModel, stepModel, fullModel, amIntWtModel)
confint(amIntWtModel) # results hidden
```
We end up selecting the model with the highest Adjusted R-squared value, "mpg ~ wt + qsec + am + wt:am".
```{r}
summary(amIntWtModel)$coef
```
Thus, the result shows that when "wt" (weight lb/1000) and "qsec" (1/4 mile time) remain constant, cars with manual transmission add 14.079 + (-4.141)*wt more MPG (miles per gallon) on average than cars with automatic transmission. That is, a manual transmitted car that weighs 2000 lbs have 5.797 more MPG than an automatic transmitted car that has both the same weight and 1/4 mile time.
## Residual Analysis and Diagnostics
Please refer to the **Appendix: Figures** section for the plots. According to the residual plots, we can verify the following underlying assumptions:
1. The Residuals vs. Fitted plot shows no consistent pattern, supporting the accuracy of the independence assumption.
2. The Normal Q-Q plot indicates that the residuals are normally distributed because the points lie closely to the line.
3. The Scale-Location plot confirms the constant variance assumption, as the points are randomly distributed.
4. The Residuals vs. Leverage argues that no outliers are present, as all values fall well within the 0.5 bands.
As for the Dfbetas, the measure of how much an observation has effected the estimate of a regression coefficient, we get the following result:
```{r}
sum((abs(dfbetas(amIntWtModel)))>1)
```
Therefore, the above analyses meet all basic assumptions of linear regression and well answer the questions.
## Appendix: Figures
1. Boxplot of MPG vs. Transmission
```{r}
boxplot(mpg ~ am, xlab="Transmission (0 = Automatic, 1 = Manual)", ylab="MPG",
main="Boxplot of MPG vs. Transmission")
```
2. Pair Graph of Motor Trend Car Road Tests
```{r}
pairs(mtcars, panel=panel.smooth, main="Pair Graph of Motor Trend Car Road Tests")
```
3. Scatter Plot of MPG vs. Weight by Transmission
```{r}
ggplot(mtcars, aes(x=wt, y=mpg, group=am, color=am, height=3, width=3)) + geom_point() +
scale_colour_discrete(labels=c("Automatic", "Manual")) +
xlab("weight") + ggtitle("Scatter Plot of MPG vs. Weight by Transmission")
```
4. Residual Plots
```{r}
par(mfrow = c(2, 2))
plot(amIntWtModel)
```
---
title: "Regression Models Course Project - Motor Trend Data Analysis Report"
author: "Alexander Loth"
output:
pdf_document:
fig_height: 6
fig_width: 7
---
## Abstract
In this report, we will analyze the `mtcars` data set and explore the relationship between a set of variables and miles per gallon (MPG). The data was extracted from the 1974 *Motor Trend* US magazine, and comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973â€“74 models). We use regression models and exploratory data analyses to mainly explore how **automatic** (am = 0) and **manual** (am = 1) transmissions features affect the **MPG** feature. The t-test shows that the performance difference between cars with automatic and manual transmission. And it is about 7 MPG more for cars with manual transmission than those with automatic transmission. Then, we fit several linear regression models and select the one with highest Adjusted R-squared value. So, given that weight and 1/4 mile time are held constant, manual transmitted cars are 14.079 + (-4.141)*weight more MPG on average better than automatic transmitted cars. Thus, cars that are lighter in weight with a manual transmission and cars that are heavier in weight with an automatic transmission will have higher MPG values.
## Exploratory Data Analysis
First, we load the `mtcars` data set and change some variables from `numeric` class to `factor` class.
```{r}
library(ggplot2)
data(mtcars)
mtcars[1:3, ] # Sample Data
dim(mtcars)
mtcars$cyl <- as.factor(mtcars$cyl)
mtcars$vs <- as.factor(mtcars$vs)
mtcars$am <- factor(mtcars$am)
mtcars$gear <- factor(mtcars$gear)
mtcars$carb <- factor(mtcars$carb)
attach(mtcars)
```
Then, we do some basic exploratory data analyses. Please refer to the **Appendix: Figures** section for the plots. According to the box plot, we see that manual transmission yields higher values of MPG in general. And as for the pair graph, we can see some higher correlations between variables like "wt", "disp", "cyl" and "hp".
## Inference
At this step, we make the null hypothesis as the MPG of the automatic and manual transmissions are from the same population (assuming the MPG has a normal distribution). We use the two sample T-test to show it.
```{r}
result <- t.test(mpg ~ am)
result$p.value
result$estimate
```
Since the p-value is 0.00137, we reject our null hypothesis. So, the automatic and manual transmissions are from different populations. And the mean for MPG of manual transmitted cars is about 7 more than that of automatic transmitted cars.
## Regression Analysis
First, we fit the full model as the following.
```{r, results='hide'}
fullModel <- lm(mpg ~ ., data=mtcars)
summary(fullModel) # results hidden
```
This model has the Residual standard error as 2.833 on 15 degrees of freedom. And the Adjusted R-squared value is 0.779, which means that the model can explain about 78% of the variance of the MPG variable. However, none of the coefficients are significant at 0.05 significant level.
Then, we use backward selection to select some statistically significant variables.
```{r, results='hide'}
stepModel <- step(fullModel, k=log(nrow(mtcars)))
summary(stepModel) # results hidden
```
This model is "mpg ~ wt + qsec + am". It has the Residual standard error as 2.459 on 28 degrees of freedom. And the Adjusted R-squared value is 0.8336, which means that the model can explain about 83% of the variance of the MPG variable. All of the coefficients are significant at 0.05 significant level.
Please refer to the **Appendix: Figures** section for the plots again. According to the scatter plot, it indicates that there appear to be an interaction term between "wt" variable and "am" variable, since automatic cars tend to weigh heavier than manual cars. Thus, we have the following model including the interaction term:
```{r, results='hide'}
amIntWtModel<-lm(mpg ~ wt + qsec + am + wt:am, data=mtcars)
summary(amIntWtModel) # results hidden
```
This model has the Residual standard error as 2.084 on 27 degrees of freedom. And the Adjusted R-squared value is 0.8804, which means that the model can explain about 88% of the variance of the MPG variable. All of the coefficients are significant at 0.05 significant level. This is a pretty good one.
Next, we fit the simple model with MPG as the outcome variable and Transmission as the predictor variable.
```{r, results='hide'}
amModel<-lm(mpg ~ am, data=mtcars)
summary(amModel) # results hidden
```
It shows that on average, a car has 17.147 mpg with automatic transmission, and if it is manual transmission, 7.245 mpg is increased. This model has the Residual standard error as 4.902 on 30 degrees of freedom. And the Adjusted R-squared value is 0.3385, which means that the model can explain about 34% of the variance of the MPG variable. The low Adjusted R-squared value also indicates that we need to add other variables to the model.
Finally, we select the final model.
```{r, results='hide'}
anova(amModel, stepModel, fullModel, amIntWtModel)
confint(amIntWtModel) # results hidden
```
We end up selecting the model with the highest Adjusted R-squared value, "mpg ~ wt + qsec + am + wt:am".
```{r}
summary(amIntWtModel)$coef
```
Thus, the result shows that when "wt" (weight lb/1000) and "qsec" (1/4 mile time) remain constant, cars with manual transmission add 14.079 + (-4.141)*wt more MPG (miles per gallon) on average than cars with automatic transmission. That is, a manual transmitted car that weighs 2000 lbs have 5.797 more MPG than an automatic transmitted car that has both the same weight and 1/4 mile time.
## Residual Analysis and Diagnostics
Please refer to the **Appendix: Figures** section for the plots. According to the residual plots, we can verify the following underlying assumptions:
1. The Residuals vs. Fitted plot shows no consistent pattern, supporting the accuracy of the independence assumption.
2. The Normal Q-Q plot indicates that the residuals are normally distributed because the points lie closely to the line.
3. The Scale-Location plot confirms the constant variance assumption, as the points are randomly distributed.
4. The Residuals vs. Leverage argues that no outliers are present, as all values fall well within the 0.5 bands.
As for the Dfbetas, the measure of how much an observation has effected the estimate of a regression coefficient, we get the following result:
```{r}
sum((abs(dfbetas(amIntWtModel)))>1)
```
Therefore, the above analyses meet all basic assumptions of linear regression and well answer the questions.
## Appendix: Figures
1. Boxplot of MPG vs. Transmission
```{r}
boxplot(mpg ~ am, xlab="Transmission (0 = Automatic, 1 = Manual)", ylab="MPG",
main="Boxplot of MPG vs. Transmission")
```
2. Pair Graph of Motor Trend Car Road Tests
```{r}
pairs(mtcars, panel=panel.smooth, main="Pair Graph of Motor Trend Car Road Tests")
```
3. Scatter Plot of MPG vs. Weight by Transmission
```{r}
ggplot(mtcars, aes(x=wt, y=mpg, group=am, color=am, height=3, width=3)) + geom_point() +
scale_colour_discrete(labels=c("Automatic", "Manual")) +
xlab("weight") + ggtitle("Scatter Plot of MPG vs. Weight by Transmission")
```
4. Residual Plots
```{r}
par(mfrow = c(2, 2))
plot(amIntWtModel)
```
library(caret)
library(rpart)
library(rpart.plot)
library(randomForest)
library(corrplot)
install.packages("corrplot")
library(corrplot)
trainUrl <-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
trainFile <- "./data/pml-training.csv"
testFile  <- "./data/pml-testing.csv"
if (!file.exists("./data")) {
dir.create("./data")
}
if (!file.exists(trainFile)) {
download.file(trainUrl, destfile=trainFile, method="curl")
}
if (!file.exists(testFile)) {
download.file(testUrl, destfile=testFile, method="curl")
}
trainRaw <- read.csv("./data/pml-training.csv")
testRaw <- read.csv("./data/pml-testing.csv")
dim(trainRaw)
dim(testRaw)
sum(complete.cases(trainRaw))
trainRaw <- trainRaw[, colSums(is.na(trainRaw)) == 0]
testRaw <- testRaw[, colSums(is.na(testRaw)) == 0]
classe <- trainRaw$classe
trainRemove <- grepl("^X|timestamp|window", names(trainRaw))
trainRaw <- trainRaw[, !trainRemove]
trainCleaned <- trainRaw[, sapply(trainRaw, is.numeric)]
trainCleaned$classe <- classe
testRemove <- grepl("^X|timestamp|window", names(testRaw))
testRaw <- testRaw[, !testRemove]
testCleaned <- testRaw[, sapply(testRaw, is.numeric)]
set.seed(22519) # For reproducibile purpose
inTrain <- createDataPartition(trainCleaned$classe, p=0.70, list=F)
trainData <- trainCleaned[inTrain, ]
testData <- trainCleaned[-inTrain, ]
set.seed(210183) # For reproducibile purpose
inTrain <- createDataPartition(trainCleaned$classe, p=0.70, list=F)
trainData <- trainCleaned[inTrain, ]
testData <- trainCleaned[-inTrain, ]
controlRf <- trainControl(method="cv", 5)
modelRf <- train(classe ~ ., data=trainData, method="rf", trControl=controlRf, ntree=250)
modelRf
predictRf <- predict(modelRf, testData)
confusionMatrix(testData$classe, predictRf)
accuracy <- postResample(predictRf, testData$classe)
accuracy
oose <- 1 - as.numeric(confusionMatrix(testData$classe, predictRf)$overall[1])
oose
result <- predict(modelRf, testCleaned[, -length(names(testCleaned))])
result
corrPlot <- cor(trainData[, -length(names(trainData))])
corrplot(corrPlot, method="color")
treeModel <- rpart(classe ~ ., data=trainData, method="class")
prp(treeModel) # fast plot
install.packages("rpart.plot")
answers <- result
pml_write_files <- function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_results/problem_id_",i,".txt")
write.table(x[i], file=filename, quote=FALSE,
row.names=FALSE, col.names=FALSE)
}
}
pml_write_files(answers)
answers <- result
pml_write_files <- function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_results/problem_id_",i,".txt")
write.table(x[i], file=filename, quote=FALSE,
row.names=FALSE, col.names=FALSE)
}
}
pml_write_files(answers)
answers <- result
pml_write_files <- function(x){
n = length(x)
for(i in 1:n){
filename = paste0("./problem_results/problem_id_",i,".txt")
write.table(x[i], file=filename, quote=FALSE,
row.names=FALSE, col.names=FALSE)
}
}
pml_write_files(answers)
answers <- result
pml_write_files <- function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_results/problem_id_",i,".txt")
write.table(x[i], file=filename, quote=FALSE,
row.names=FALSE, col.names=FALSE)
}
}
pml_write_files(answers)
answers <- result
pml_write_files <- function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_results/problem_id_",i,".txt")
write.table(x[i], file=filename, quote=FALSE,
row.names=FALSE, col.names=FALSE)
}
}
pml_write_files(answers)
answers <- result
pml_write_files <- function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_results/problem_id_",i,".txt")
write.table(x[i], file=filename, quote=FALSE,
row.names=FALSE, col.names=FALSE)
}
}
testing_data <- testing_data[featureset[featureset!='classe']]
answers <- predict(model, newdata=testing_data)
answers
pml_write_files(answers)
answers <- result
pml_write_files <- function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_results/problem_id_",i,".txt")
write.table(x[i], file=filename, quote=FALSE,
row.names=FALSE, col.names=FALSE)
}
}
answers
pml_write_files(answers)
answers <- result
pml_write_files <- function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
write.table(x[i], file=filename, quote=FALSE,
row.names=FALSE, col.names=FALSE)
}
}
answers
pml_write_files(answers)
install.packages('devtools')
devtools::install_github('rstudio/shinyapps')
shinyapps::setAccountInfo(name='alexloth', token='48D25B8C044C8E43C24A6455ACB8374A', secret='PD1ir9HxcDyt6+Fg8f2VDY5Q7HzFn/E05nY9lL6m')
devtools::install_github('rstudio/rsconnect')
rsconnect::setAccountInfo
setwd("~/")
library(shiny)
runApp()
install.packages('shiny')
library(shiny)
runApp()
setwd("~/OneDrive/Coursera/Data Science Specialization/09 Developing Data Products/3-1 Assignment 1")
library(shiny)
runApp()
install.packages("quantmod")
runApp()
install.packages('VGAM')
runApp()
runApp()
runApp()
install.packages('TTR')
install.packages("TTR")
install.packages('TTR')
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
install.packages("PerformanceAnalytics")
shiny::runApp()
shiny::runApp()
shiny::runApp()
names(quantmod:::.chart.theme)
shiny::runApp()
names(quantmod:::.chart.type)
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
